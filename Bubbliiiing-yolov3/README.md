# YOLO-V3:
>使用了特征金字塔结构和主干特征网络提取->提取特征

BackBone:DarkNet-53

输入是416x416X3->DarkNet-53输出是13x13x1024，不断进行下采样，获取到的每个特征图大小为13x13x256，总共有52层，可以表示输入进入的图片的特征
对于最后三个特征层：
52,52,256->52x52x75->52x52x3x25->52x52x3x20+1+4
26,26,512-> 会和13x13x256的上采样进行concat，输出为26x26x768->进行五次卷积，然后就和下面操作相似->26x26x20+1+4
13,13,1024->5次卷积，输出为13x13x75=13x13x3x25=13x13x3(三个先验框)x20(20个类的概率)+1(判断先验框里面是否有物体)+4(先验框的四个参数)

下采样会降低分辨率和增加通道数

非极大值抑制越小检测框更严格，保留的框更少


# 相关知识点

Recall（召回率）：衡量“检测到多少真实目标”，低召回率 = 漏检多。

F1-score：衡量“精确率和召回率的平衡”，F1 越高，模型综合性能越好。

残差网络（Residual Network，简称ResNet）是一种深度卷积神经网络架构，由微软研究院的何恺明等人于2015年提出。其核心创新是通过引入残差学习（Residual Learning）和跳跃连接（Skip Connection），有效解决了深度神经网络中的梯度消失/爆炸和网络退化问题，使得训练极深的网络（如超过100层）成为可能。

核心思想
残差块（Residual Block）：

传统网络直接学习目标映射 H(x)H(x)，而残差网络改为学习残差 F(x) = H(x) - xF(x)=H(x)−x。
通过跳跃连接将输入 xx 与卷积层输出 F(x)F(x) 相加，得到最终输出 H(x) = F(x) + xH(x)=F(x)+x。
这种设计让网络更容易学习恒等映射（即 F(x) \approx 0F(x)≈0），避免深层网络的性能退化。
跳跃连接（Shortcut Connection）：

跨层直接传递输入，缓解梯度反向传播时的逐层衰减问题。
若输入与输出的维度不同，可通过1x1卷积调整通道数或步长（stride）调整空间尺寸。
关键优势
训练更深的网络：如ResNet-152（152层），而传统网络（如VGG）通常不超过20层。
缓解梯度问题：梯度可通过跳跃连接直接回传，减少消失/爆炸风险。
性能提升：在ImageNet等数据集上错误率显著降低（如ResNet-34相比VGG-19错误率更低）。

非极大抑制（Non-Maximum Suppression, NMS）是目标检测和图像处理中的关键后处理步骤，主要用于消除冗余的检测框，保留最可能代表真实目标的候选框。其核心作用如下：

1. 解决重复检测问题
问题背景：目标检测模型（如Faster R-CNN、YOLO等）会对同一目标生成多个重叠的预测框（不同置信度或位置）。
NMS的作用：通过抑制置信度较低的重叠框，仅保留最准确的预测框，避免同一目标被多次检测。
2. 核心原理
步骤：
按置信度从高到低排序所有检测框。
选择最高置信度的框，保留它。
计算该框与其他所有框的重叠程度（如IoU，交并比）。
删除与保留框IoU超过设定阈值（如0.5）的其他框。
重复上述过程，直到处理完所有框。
3. 实际应用场景
目标检测：如人脸检测、车辆检测中，避免同一目标被多个框标记。
关键点检测：去除重复的关键点预测。
文本检测：防止同一文字区域被多次框选。
4. 优势
提升结果清晰度：输出无重叠的干净检测结果。
降低计算负担：减少后续处理（如跟踪、分类）的冗余数据。
5. 改进与变体
Soft-NMS：通过降低重叠框的置信度而非直接删除，缓解密集目标漏检问题。
IoU-NMS：根据任务调整IoU阈值（如严格阈值用于人脸检测，宽松阈值用于行人检测）。
示例
假设检测到同一只狗的3个框（置信度0.9、0.7、0.6），若IoU阈值设为0.5，NMS会保留0.9的框，其余被抑制。

非极大抑制是目标检测流程中不可或缺的一环，直接影响结果的准确性和可用性。