{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de817138",
   "metadata": {},
   "source": [
    "# 部署你的第一个语言模型。\n",
    "在之前的章节中，我们已经了解 Hugging Face 中 AutoModel 系列的不同类。现在，我们将使用一个参数量较小的模型进行演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e1dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# 精简版GPT-2模型\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee166bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把模型移到GPU上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d986da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "束宽 1 的生成结果：\n",
      "Hello GPT is a free and open source software project that aims to provide a platform for developers to build and use GPGP-based GPSP based GPCs. GPP is an open-source software development platform that is designed to allow developers and developers of GGPP to use the GPRP protocol.\n",
      "\n",
      "GPP has been developed by the OpenPGp Foundation and is based on the open standards of the GNU/Linux Foundation. The GP Protocol is the standard protocol for GPs that are used by GIPP and GSPP. It is also the protocol used for the use of a GTP protocol, GSMTP, and other GMPP protocols.\n",
      "--------------------------------------------------\n",
      "束宽 3 的生成结果：\n",
      "Hello GPT.\n",
      "\n",
      "This article is part of a series of articles on the topic, and will be updated as more information becomes available.\n",
      "--------------------------------------------------\n",
      "束宽 5 的生成结果：\n",
      "Hello GPT.\n",
      "\n",
      "This article was originally published on The Conversation. Read the original article.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 进行推理\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "input_text=\"Hello GPT\"\n",
    "# 编码输入文本\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "inputs= {key: value.to(device) for key, value in inputs.items()}\n",
    "beam_width=[1,3,5]\n",
    "# 生成文本\n",
    "for beam in beam_width:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, # 输入\n",
    "            max_length=200, # 最大长度\n",
    "            num_beams=beam, # 最优的 5 个候选序列,Beam Search 的数量，提高生成文本的质量\n",
    "            no_repeat_ngram_size=2, # 禁止重复的n-gram大小,防止生成重复的 n-gram\n",
    "            early_stopping=True # 早停,当使用 Beam Search 时，若所有候选序列都生成了结束标记（如 <eos>），则提前停止生成，这有助于生成更自然和适当长度的文本。\n",
    "        )\n",
    "# 解码输出文本\n",
    "    generated_text=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"束宽 {beam} 的生成结果：\")\n",
    "    print(generated_text)\n",
    "    print(\"-\"*50)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
